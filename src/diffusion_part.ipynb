{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b74Nmough8wu"
      },
      "source": [
        "# Deep Learning Project, Part 1\n",
        "\n",
        "This notebook is our first part. Training our diffusion model using SHHQ dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZve_fTgvTpz"
      },
      "outputs": [],
      "source": [
        "# For displaying progress bars during training loops\n",
        "!pip -q install tqdm\n",
        "\n",
        "# Standard Python utilities\n",
        "import os, math, random\n",
        "from glob import glob\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# PyTorch core library\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Dataset and DataLoader utilities\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Image preprocessing and transformations\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid, save_image\n",
        "\n",
        "# Image loading and handling\n",
        "from PIL import Image\n",
        "\n",
        "# Progress bar\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Object-oriented file system paths\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xQtVX3cevTso",
        "outputId": "223f31ee-eb42-4552-d8ec-bba55d11a04b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpWQPuZGwsAX",
        "outputId": "0adaa972-2d60-4d9a-a0da-766ec91486a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# We need to connect drive to get data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IHhP25g6yhFB"
      },
      "outputs": [],
      "source": [
        "# We are unzipping the file located in the drive as a zip file\n",
        "#!unzip -P StylisH-HumanS-hq_1.0 \"/content/drive/MyDrive/SHHQ-1.0.zip\" -d \"/content/drive/MyDrive/SHHQ_1.0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eA0NtjIs6uk"
      },
      "outputs": [],
      "source": [
        "img_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 64)), # All images are resized to 128,64 dimensions\n",
        "    transforms.ToTensor(),  # Converted to tensor\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], # Since the -1.1 range is required for diffusion, we perform that transformation\n",
        "                         std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "mask_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 64)),\n",
        "    transforms.ToTensor(),   # There is no normalization because mask is information, not noise\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWnwsLx6vTu-"
      },
      "outputs": [],
      "source": [
        "class ImageMaskDataset(Dataset):\n",
        "  \"\"\"\n",
        "    Custom PyTorch Dataset for loading paired human images and segmentation masks.\n",
        "    Each sample consists of: An RGB image, a corresponding binary segmentation mask.\n",
        "    \"\"\"\n",
        "\n",
        "  def __init__(self, img_dir, mask_dir, img_transform=None, mask_transform=None):\n",
        "      \"\"\"\n",
        "        Args:\n",
        "            img_dir (str): Directory containing RGB input images.\n",
        "            mask_dir (str): Directory containing corresponding segmentation masks.\n",
        "            img_transform (callable, optional): Transformations applied to images.\n",
        "            mask_transform (callable, optional): Transformations applied to masks.\n",
        "        \"\"\"\n",
        "      self.img_dir = img_dir\n",
        "      self.mask_dir = mask_dir\n",
        "      self.img_transform = img_transform\n",
        "      self.mask_transform = mask_transform\n",
        "\n",
        "      # Collect and sort image filenames to ensure deterministic pairing\n",
        "      self.img_files = sorted([\n",
        "          f for f in os.listdir(img_dir)\n",
        "          if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
        "      ])\n",
        "\n",
        "      # Collect and sort mask filenames\n",
        "      self.mask_files = sorted([\n",
        "          f for f in os.listdir(mask_dir)\n",
        "          if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
        "      ])\n",
        "\n",
        "      # Print dataset statistics\n",
        "      print(\"Images:\", len(self.img_files))\n",
        "      print(\"Masks :\", len(self.mask_files))\n",
        "\n",
        "\n",
        "      assert len(self.img_files) == len(self.mask_files)\n",
        "\n",
        "  def __len__(self):\n",
        "      \"\"\"\n",
        "      Returns:\n",
        "          int: Total number of samples in the dataset.\n",
        "      \"\"\"\n",
        "      return len(self.img_files)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      \"\"\"\n",
        "      Loads and returns a single (image, mask) pair.\n",
        "\n",
        "      Args:\n",
        "          idx (int): Index of the sample.\n",
        "\n",
        "      Returns:\n",
        "          img (Tensor): Transformed RGB image tensor.\n",
        "          mask (Tensor): Binary segmentation mask tensor.\n",
        "      \"\"\"\n",
        "\n",
        "      # Load RGB image and convert to 3-channel format\n",
        "      img = Image.open(os.path.join(self.img_dir, self.img_files[idx])).convert(\"RGB\")\n",
        "\n",
        "      # Load mask image and convert to single-channel (grayscale)\n",
        "      mask = Image.open(os.path.join(self.mask_dir, self.mask_files[idx])).convert(\"L\")\n",
        "\n",
        "      # Apply image transformations\n",
        "      if self.img_transform:\n",
        "          img = self.img_transform(img)\n",
        "\n",
        "      # Apply mask transformations\n",
        "      if self.mask_transform:\n",
        "          mask = self.mask_transform(mask)\n",
        "\n",
        "      # Convert mask to strict binary format (0 or 1)\n",
        "      # This ensures a clean silhouette for conditioning\n",
        "      mask = (mask > 0.5).float()\n",
        "\n",
        "      return img, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyHHBih0uHNz"
      },
      "outputs": [],
      "source": [
        "# Since it takes too long for the model to pull all the data from the drive and slows down the model, we upload the data to Colab\n",
        "!cp /content/drive/MyDrive/SHHQ-1.0.zip /content/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Je4g0s9xeQQA",
        "outputId": "4113c88a-91b7-4095-af17-b83ca4ae713f"
      },
      "outputs": [],
      "source": [
        "# Unzip in colab\n",
        "!unzip -P StylisH-HumanS-hq_1.0 /content/SHHQ-1.0.zip -d /content/data/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvF_0c29zMy_",
        "outputId": "bf6e9bd9-5423-4894-d033-0099a3b0cbbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Images: 40000\n",
            "Masks : 40000\n"
          ]
        }
      ],
      "source": [
        "img_dir  = \"/content/data/SHHQ-1.0/no_segment\" # Original images\n",
        "mask_dir = \"/content/data/SHHQ-1.0/segments\" # Masks\n",
        "\n",
        "# Initialize the custom dataset that pairs each image with its segmentation mask\n",
        "dataset = ImageMaskDataset(\n",
        "    img_dir=img_dir,\n",
        "    mask_dir=mask_dir,\n",
        "    img_transform=img_transform,\n",
        "    mask_transform=mask_transform\n",
        ")\n",
        "\n",
        "# Create a DataLoader for efficient mini-batch training\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,  # Shuffle data to improve generalization\n",
        "    num_workers=2,  # Parallel data loading using CPU workers\n",
        "    pin_memory=True  # Faster host-to-GPU memory transfer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fErzdeZWzM1K"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This class defines the forward diffusion process in a DDPM.\n",
        "\n",
        "In diffusion models, there are two main processes:\n",
        "1) Forward diffusion  : Gradually add Gaussian noise to a clean image\n",
        "2) Reverse diffusion  : Learn to remove the noise step by step to recover the image\n",
        "\n",
        "This scheduler only handles the forward diffusion process.\n",
        "\n",
        "The task of this class: \"To mathematically define how to distort an image in a controlled manner.\"\n",
        "'''\n",
        "class DDPMScheduler:\n",
        "    def __init__(\n",
        "        self,\n",
        "        T=1000, # Total number of diffusion steps\n",
        "        beta_start=1e-4, # Initial noise level\n",
        "        beta_end=0.015,   # Final noise level\n",
        "        device=\"cuda\"\n",
        "    ):\n",
        "\n",
        "        # Number of diffusion timesteps\n",
        "        self.T = T\n",
        "\n",
        "        # Linearly spaced noise schedule (beta_t)\n",
        "        # Each beta controls how much noise is added at step t\n",
        "        self.betas = torch.linspace(\n",
        "            beta_start,\n",
        "            beta_end,\n",
        "            T,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # Alpha_t = 1 - beta_t\n",
        "        # Represents how much signal remains after adding noise at step t\n",
        "        self.alphas = 1.0 - self.betas\n",
        "\n",
        "        # Cumulative product of alphas\n",
        "        # This represents how much of the original image remains after t steps\n",
        "        self.alpha_bar = torch.cumprod(self.alphas, dim=0)\n",
        "\n",
        "        # Precompute square roots for efficiency\n",
        "        self.sqrt_alpha_bar = torch.sqrt(self.alpha_bar)\n",
        "        self.sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bar)\n",
        "\n",
        "    def q_sample(self, x0, t, noise):\n",
        "        \"\"\"\n",
        "        Forward diffusion equation:\n",
        "        x_t = sqrt(alpha_bar_t) * x_0 + sqrt(1 - alpha_bar_t) * noise\n",
        "\n",
        "        This function takes a clean image x0 and returns a noisy version x_t\n",
        "        at timestep t.\n",
        "        \"\"\"\n",
        "\n",
        "        B = x0.size(0) # Batch size\n",
        "\n",
        "        # Select alpha_bar_t for each sample in the batch\n",
        "        a = self.sqrt_alpha_bar[t].view(B, 1, 1, 1)\n",
        "        b = self.sqrt_one_minus_alpha_bar[t].view(B, 1, 1, 1)\n",
        "\n",
        "        # Combine the original image and Gaussian noise\n",
        "        # As t increases, the image becomes more noisy\n",
        "        return a * x0 + b * noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FcKSdMNzM3g"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This function encodes the diffusion timestep (t) into a high-dimensiona vector representation. It allows the model to know which diffusion step\n",
        "it is currently processing.\n",
        "\n",
        "In diffusion models, the same network is used for all timesteps, so explicit time conditioning is required.\n",
        "\"\"\"\n",
        "def sinusoidal_embedding(t, dim):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "      t (Tensor): Diffusion timestep tensor of shape (B,)\n",
        "      dim (int): Embedding dimension used to represent the timestep. Larger dimensions provide richer temporal information.\n",
        "                  We use 256 as a practical and commonly adopted choice in diffusion literature.\n",
        "\n",
        "  Returns:\n",
        "      Tensor: Sinusoidal time embeddings of shape (B, dim)\n",
        "  \"\"\"\n",
        "\n",
        "  # Half of the embedding is used for sine, half for cosine\n",
        "  half = dim // 2\n",
        "\n",
        "  # Generate exponentially decreasing frequencies\n",
        "  # This allows the embedding to capture both\n",
        "  # low-frequency (coarse) and high-frequency (fine) time information\n",
        "  freqs = torch.exp(\n",
        "      -math.log(10000) * torch.arange(half, device=t.device) / (half - 1)\n",
        "  )\n",
        "\n",
        "  # Scale frequencies by the timestep\n",
        "  args = t.float().unsqueeze(1) * freqs.unsqueeze(0)\n",
        "\n",
        "  # Concatenate sine and cosine embeddings\n",
        "  # This makes the representation smooth and uniquely decodable\n",
        "  emb = torch.cat([torch.sin(args), torch.cos(args)], dim=1)\n",
        "\n",
        "  # These time embeddings are injected into every ResBlock so the network is explicitly aware that:\n",
        "  # \"this feature map was generated at diffusion step t\"\n",
        "  return emb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHuLQaM9Bs3S"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This block is the fundamental building unit of the U-Net architecture.\n",
        "It processes spatial features while explicitly conditioning on the diffusion timestep through time embeddings.\n",
        "\n",
        "Residual connections help stabilize training and preserve information, which is especially important in deep diffusion models.\n",
        "'''\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, t_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            in_ch (int): Number of input feature channels.\n",
        "            out_ch (int): Number of output feature channels.\n",
        "            t_dim (int): Dimensionality of the time embedding (e.g., 256).\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Group Normalization stabilizes training for small batch sizes, which is common in diffusion models\n",
        "        self.norm1 = nn.GroupNorm(8, in_ch)\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
        "\n",
        "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "\n",
        "        # Linear projection that maps the time embedding to the same dimensionality as the feature channels\n",
        "        self.time_mlp = nn.Linear(t_dim, out_ch)\n",
        "\n",
        "        # Skip connection to preserve information and enable residual learning. If channel dimensions differ, a 1×1 convolution aligns them\n",
        "        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x, t_emb):\n",
        "        \"\"\"\n",
        "        Forward pass with time conditioning.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input feature map of shape (B, C, H, W).\n",
        "            t_emb (Tensor): Time embedding of shape (B, t_dim).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Output feature map with residual connection applied.\n",
        "        \"\"\"\n",
        "\n",
        "        # First normalization + nonlinearity + convolution\n",
        "        h = self.conv1(F.silu(self.norm1(x)))\n",
        "\n",
        "        # Inject time information into the feature map\n",
        "        # This allows the network to adapt its behavior depending on the current diffusion step\n",
        "        time = self.time_mlp(t_emb).view(x.size(0), -1, 1, 1)\n",
        "        h = h + time\n",
        "\n",
        "        # Second normalization + nonlinearity + convolution\n",
        "        h = self.conv2(F.silu(self.norm2(h)))\n",
        "\n",
        "        # Residual connection ensures stable gradient flow\n",
        "        return h + self.skip(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KENQOaqVBtjP"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This block reduces the spatial resolution of the feature maps.\n",
        "It allows the network to capture global structure by increasing the receptive field.\n",
        "\n",
        "In diffusion U-Nets, downsampling is crucial for modeling long range spatial dependencies efficiently.\n",
        "'''\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, ch):\n",
        "      \"\"\"\n",
        "        Args:\n",
        "            ch (int): Number of feature channels.\n",
        "      \"\"\"\n",
        "\n",
        "      super().__init__()\n",
        "\n",
        "      # Strided convolution halves the spatial resolution while preserving the number of channels\n",
        "      self.conv = nn.Conv2d(ch, ch, 4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): Input feature map of shape (B, C, H, W).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Downsampled feature map (H/2, W/2).\n",
        "        \"\"\"\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "'''\n",
        "This block increases the spatial resolution of the feature maps.\n",
        "It reconstructs fine-grained spatial details using information from earlier layers via skip connections.\n",
        "\n",
        "Upsampling is essential for recovering image details after aggressive downsampling in the encoder.\n",
        "'''\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, ch):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            ch (int): Number of feature channels.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Transposed convolution doubles the spatial resolution and prepares features for concatenation with skip connections\n",
        "        self.tconv = nn.ConvTranspose2d(ch, ch, 4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): Input feature map of shape (B, C, H, W).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Upsampled feature map (2H, 2W).\n",
        "        \"\"\"\n",
        "        return self.tconv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yz4hlvgXPbJB"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This block enables the model to capture long-range spatial dependencies.\n",
        "Unlike convolution, which is inherently local, self-attention allows every spatial location to directly interact with every other location.\n",
        "\n",
        "In diffusion models, self-attention is especially useful for maintaining global coherence, such as consistent body structure and symmetry.\n",
        "'''\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            channels (int): Number of feature channels.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Normalize features before computing attention GroupNorm is used for stability with small batch sizes\n",
        "        self.norm = nn.GroupNorm(32, channels)\n",
        "\n",
        "        # Linear projections to obtain Query, Key, and Value tensors\n",
        "        # 1×1 convolutions preserve spatial dimensions while mixing channels\n",
        "        self.q = nn.Conv2d(channels, channels, 1)\n",
        "        self.k = nn.Conv2d(channels, channels, 1)\n",
        "        self.v = nn.Conv2d(channels, channels, 1)\n",
        "\n",
        "        # Output projection after attention\n",
        "        self.proj = nn.Conv2d(channels, channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (Tensor): Input feature map of shape (B, C, H, W).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Feature map enhanced with global contextual information.\n",
        "        \"\"\"\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        # Normalize input features\n",
        "        h = self.norm(x)\n",
        "\n",
        "        # Compute query, key, and value tensors\n",
        "        # Flatten spatial dimensions (H × W)\n",
        "        q = self.q(h).reshape(B, C, H * W)\n",
        "        k = self.k(h).reshape(B, C, H * W)\n",
        "        v = self.v(h).reshape(B, C, H * W)\n",
        "\n",
        "        # Compute attention matrix\n",
        "        # Each spatial location attends to all other locations\n",
        "        attn = torch.softmax(\n",
        "            torch.bmm(q.permute(0, 2, 1), k) / (C ** 0.5),\n",
        "            dim=-1\n",
        "        )\n",
        "\n",
        "        out = torch.bmm(v, attn.permute(0, 2, 1))\n",
        "        out = out.reshape(B, C, H, W)\n",
        "\n",
        "        return x + self.proj(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lE_3SG4Btlk"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "U-Net is well-suited for diffusion models because it can capture both global structure (e.g., full human silhouette)\n",
        "and local details (e.g., arms, legs, edges) simultaneously.\n",
        "\n",
        "The encoder extracts hierarchical features at multiple resolutions, while the decoder reconstructs fine details using skip connections.\n",
        "'''\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch=4, base=64, t_dim=256):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            in_ch (int): Number of input channels.\n",
        "                          (3 RGB channels + 1 segmentation mask channel)\n",
        "            base (int): Base number of feature channels.\n",
        "            t_dim (int): Dimensionality of the time embedding.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Time embedding MLP\n",
        "        # Transforms the sinusoidal time embedding\n",
        "        # so it can be injected into ResBlocks\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.Linear(t_dim, t_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(t_dim, t_dim)\n",
        "        )\n",
        "\n",
        "        # Initial convolution\n",
        "        # Maps the input (RGB image + mask) to base feature channels\n",
        "        self.in_conv = nn.Conv2d(in_ch, base, 3, padding=1)# 3 kanaldan(RGB), 64 kanala çevirdik conv ile\n",
        "\n",
        "        # Encoder (Downsampling)\n",
        "        # As depth increases: Spatial resolution decreases, feature channels increase\n",
        "        # This allows the network to capture global structure efficiently\n",
        "\n",
        "        self.rb1 = ResBlock(base, base, t_dim)\n",
        "        self.down1 = Down(base) # downsampling\n",
        "\n",
        "        self.rb2 = ResBlock(base, base*2, t_dim)\n",
        "        self.down2 = Down(base*2)\n",
        "\n",
        "        self.rb3 = ResBlock(base*2, base*4, t_dim)\n",
        "        self.down3 = Down(base*4)\n",
        "\n",
        "        # Bottleneck\n",
        "        # Lowest resolution representation\n",
        "        # Responsible for capturing global context\n",
        "        self.mid1 = ResBlock(base*4, base*4, t_dim)\n",
        "\n",
        "        # Self-attention is disabled here (Identity)\n",
        "        # It was tested but removed due to high computational cost\n",
        "        self.mid_attn = nn.Identity()\n",
        "\n",
        "        self.mid2 = ResBlock(base*4, base*4, t_dim)\n",
        "\n",
        "        # Decoder (Upsampling)\n",
        "        # Gradually restores spatial resolution\n",
        "        # Skip connections inject high-resolution details\n",
        "        self.up3 = Up(base*4)\n",
        "        self.urb3 = ResBlock(base*8, base*2, t_dim)\n",
        "\n",
        "        self.up2 = Up(base*2)\n",
        "        self.urb2 = ResBlock(base*4, base, t_dim)\n",
        "\n",
        "        self.up1 = Up(base)\n",
        "        self.urb1 = ResBlock(base*2, base, t_dim)\n",
        "\n",
        "        # Final output layer\n",
        "        # Predicts noise with the same spatial size as the input image\n",
        "        self.out = nn.Conv2d(base, 3, 3, padding=1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "      \"\"\"\n",
        "        Forward pass of the U-Net.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape (B, 4, H, W)\n",
        "                        (noisy RGB image + segmentation mask)\n",
        "            t (Tensor): Diffusion timestep tensor of shape (B,)\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Predicted noise tensor of shape (B, 3, H, W)\n",
        "      \"\"\"\n",
        "\n",
        "      # Time embedding\n",
        "      # Encode diffusion step information\n",
        "      t_emb = sinusoidal_embedding(t, 256)\n",
        "      t_emb = self.time_mlp(t_emb)\n",
        "\n",
        "      # Input conv\n",
        "      x0 = self.in_conv(x)\n",
        "\n",
        "      # Downsampling path\n",
        "      h1 = self.rb1(x0, t_emb)\n",
        "      d1 = self.down1(h1)\n",
        "\n",
        "      h2 = self.rb2(d1, t_emb)\n",
        "      d2 = self.down2(h2)\n",
        "\n",
        "      h3 = self.rb3(d2, t_emb)\n",
        "      d3 = self.down3(h3)\n",
        "\n",
        "      # Bottleneck\n",
        "      m = self.mid1(d3, t_emb)\n",
        "      m = self.mid_attn(m)\n",
        "      m = self.mid2(m, t_emb)\n",
        "\n",
        "      # Upsampling path\n",
        "      u3 = self.up3(m)\n",
        "      u3 = self.urb3(torch.cat([u3, h3], 1), t_emb)\n",
        "\n",
        "      u2 = self.up2(u3)\n",
        "      u2 = self.urb2(torch.cat([u2, h2], 1), t_emb)\n",
        "\n",
        "      u1 = self.up1(u2)\n",
        "      u1 = self.urb1(torch.cat([u1, h1], 1), t_emb)\n",
        "\n",
        "      # Output (noise prediction)\n",
        "      return self.out(u1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC1YIeerBtoK"
      },
      "outputs": [],
      "source": [
        "model = UNet().to(device) # Initialize the U-Net model\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
        "\n",
        "# Initialize the diffusion scheduler\n",
        "scheduler = DDPMScheduler(\n",
        "    T=1000,\n",
        "    beta_start=1e-4,\n",
        "    beta_end=0.015,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "e5suL8D5qz8W",
        "outputId": "449963b2-3b0a-4788-ada5-b7fcfd6def65"
      },
      "outputs": [],
      "source": [
        "# Initialize a separate U-Net for EMA tracking\n",
        "ema_model = UNet(\n",
        "    in_ch=4,\n",
        "    base=64,\n",
        "    t_dim=256\n",
        ").to(device)\n",
        "\n",
        "# At initialization, copy the main model weights into the EMA model\n",
        "ema_model.load_state_dict(model.state_dict())\n",
        "ema_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Glozy6aRq4sM"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def ema_update(ema_model, model, decay=0.999):\n",
        "  '''\n",
        "  This function updates the EMA model by smoothly averaging the current model parameters over time.\n",
        "\n",
        "  Instead of copying weights directly, EMA applies a weighted update:\n",
        "    Old EMA weights are kept with high importance (decay)\n",
        "    New model weights are added with a small contribution (1 - decay)\n",
        "  '''\n",
        "  for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
        "      ema_param.data.mul_(decay).add_(param.data, alpha=1 - decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2N7zBfChB1YT"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def sample_cond(model, sched, mask, n=None):\n",
        "    '''\n",
        "    This function generates images using the learned reverse diffusion process.\n",
        "    Starting from pure Gaussian noise, the model iteratively removes noise step-by-step while being guided by a segmentation mask.\n",
        "    '''\n",
        "    # Use evaluation mode for stable inference\n",
        "    model.eval()\n",
        "\n",
        "    # If sample count is not specified, use batch size from mask\n",
        "    if n is None:\n",
        "        n = mask.size(0)\n",
        "\n",
        "    # Initialize with pure Gaussian noise\n",
        "    B, _, H, W = mask.shape\n",
        "    x = torch.randn(B, 3, H, W).to(device)\n",
        "\n",
        "    # Reverse diffusion: iterate from T → 0\n",
        "    for t in reversed(range(sched.T)):\n",
        "        t_batch = torch.full((B,), t, device=device)\n",
        "\n",
        "        # Concatenate noisy image with segmentation mask\n",
        "        # This conditions generation on human silhouette\n",
        "        model_in = torch.cat([x, mask], dim=1)\n",
        "\n",
        "        # Predict noise component at timestep t\n",
        "        eps = model(model_in, t_batch)\n",
        "\n",
        "        # Retrieve scheduler coefficients\n",
        "        alpha = sched.alphas[t]\n",
        "        alpha_bar = sched.alpha_bar[t]\n",
        "        beta = sched.betas[t]\n",
        "\n",
        "        # Compute mean of the reverse diffusion distribution\n",
        "        mean = (1 / torch.sqrt(alpha)) * (\n",
        "            x - (1 - alpha) / torch.sqrt(1 - alpha_bar) * eps\n",
        "        )\n",
        "\n",
        "        # Add noise except for the final step\n",
        "        if t > 0:\n",
        "            x = mean + torch.sqrt(beta) * torch.randn_like(x)\n",
        "        else:\n",
        "            x = mean\n",
        "\n",
        "    # Final denoised image\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFkVY5nKAozO"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def ddim_sample_cond(\n",
        "    model,\n",
        "    scheduler,\n",
        "    mask,\n",
        "    device=\"cuda\",\n",
        "    ddim_steps=250,\n",
        "    eta=0.0\n",
        "):\n",
        "    # Use evaluation mode for inference\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize with pure Gaussian noise\n",
        "    B, _, H, W = mask.shape\n",
        "    x = torch.randn(B, 3, H, W, device=device)\n",
        "\n",
        "    # Select a reduced set of timesteps (e.g., 1000 → 250)\n",
        "    # This speeds up sampling significantly\n",
        "    timesteps = torch.linspace(\n",
        "        scheduler.T - 1,\n",
        "        0,\n",
        "        ddim_steps,\n",
        "        dtype=torch.long,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Reverse diffusion with fewer steps\n",
        "    for i in range(len(timesteps) - 1):\n",
        "        t = timesteps[i]\n",
        "        t_next = timesteps[i + 1]\n",
        "\n",
        "        t_batch = torch.full((B,), t, device=device)\n",
        "\n",
        "        # Concatenate noisy image with segmentation mask\n",
        "        model_in = torch.cat([x, mask], dim=1)\n",
        "\n",
        "        # Predict noise at timestep t\n",
        "        eps = model(model_in, t_batch)\n",
        "\n",
        "        # Retrieve cumulative noise coefficients\n",
        "        alpha_bar_t = scheduler.alpha_bar[t]\n",
        "        alpha_bar_next = scheduler.alpha_bar[t_next]\n",
        "\n",
        "        # Predict the clean image x0\n",
        "        x0_pred = (\n",
        "            (x - torch.sqrt(1 - alpha_bar_t) * eps)\n",
        "            / torch.sqrt(alpha_bar_t)\n",
        "        ).clamp(-1, 1)\n",
        "\n",
        "        # Control stochasticity (eta = 0 → deterministic)\n",
        "        sigma = (\n",
        "            eta\n",
        "            * torch.sqrt(\n",
        "                (1 - alpha_bar_next)\n",
        "                / (1 - alpha_bar_t)\n",
        "                * (1 - alpha_bar_t / alpha_bar_next)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        noise = sigma * torch.randn_like(x) if eta > 0 else 0\n",
        "\n",
        "        # DDIM update step\n",
        "        x = (\n",
        "            torch.sqrt(alpha_bar_next) * x0_pred\n",
        "            + torch.sqrt(1 - alpha_bar_next - sigma**2) * eps\n",
        "            + noise\n",
        "        )\n",
        "\n",
        "    # Final generated image\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "collapsed": true,
        "id": "owpdZak7ByRk",
        "outputId": "a175f05f-7667-4987-ffdc-5c21b5146353"
      },
      "outputs": [],
      "source": [
        "# Directories for saving checkpoints and generated samples\n",
        "checkpoint_dir = Path(\"/content/drive/MyDrive/checkpoints\")\n",
        "sample_dir = Path(\"/content/drive/MyDrive/samples1\")\n",
        "checkpoint_dir.mkdir(exist_ok=True)\n",
        "sample_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Training configuration\n",
        "NUM_EPOCHS = 50      # Total number of training epochs\n",
        "SAVE_EVERY = 10       # Save model checkpoint every N epochs\n",
        "SAMPLE_EVERY = 5      # Generate samples every N epochs\n",
        "\n",
        "# Track average loss per epoch\n",
        "epoch_losses = []\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    print(f\"\\n Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "\n",
        "    # Enable training mode\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    # Progress bar for batches\n",
        "    pbar = tqdm(\n",
        "        loader,\n",
        "        total=len(loader),\n",
        "        desc=f\"Epoch {epoch+1}\",\n",
        "        leave=True\n",
        "    )\n",
        "\n",
        "    for batch_idx, (x0, mask) in enumerate(pbar):\n",
        "        x0 = x0.to(device)\n",
        "        mask = mask.to(device)\n",
        "\n",
        "        # Randomly sample diffusion timestep\n",
        "        t = torch.randint(0, scheduler.T, (x0.size(0),), device=device)\n",
        "\n",
        "        # Forward diffusion: add noise to clean image\n",
        "        noise = torch.randn_like(x0)\n",
        "        xt = scheduler.q_sample(x0, t, noise)\n",
        "\n",
        "        # Predict noise using mask-conditioned U-Net\n",
        "        model_in = torch.cat([xt, mask], dim=1)\n",
        "        pred_noise = model(model_in, t)\n",
        "\n",
        "        # Face-weighted loss to emphasize perceptually important regions\n",
        "        B, _, H, W = mask.shape\n",
        "        weight_map = torch.ones_like(mask)\n",
        "        face_region = int(0.3 * H)\n",
        "        weight_map[:, :, :face_region, :] = 2.5\n",
        "        weight_map = 1.0 + mask * weight_map\n",
        "\n",
        "        # Combined MSE + L1 loss\n",
        "        diff = pred_noise - noise\n",
        "        loss = (\n",
        "            0.6 * (diff**2 * weight_map).mean() +\n",
        "            0.4 * (diff.abs() * weight_map).mean()\n",
        "        )\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update EMA model\n",
        "        ema_update(ema_model, model, decay=0.9997)\n",
        "\n",
        "        # Track loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({\n",
        "            \"loss\": f\"{loss.item():.4f}\",\n",
        "            \"avg\": f\"{epoch_loss/(batch_idx+1):.4f}\"\n",
        "        })\n",
        "\n",
        "    # Compute average epoch loss\n",
        "    avg_epoch_loss = epoch_loss / len(loader)\n",
        "    epoch_losses.append(avg_epoch_loss)\n",
        "\n",
        "    print(f\" Epoch {epoch+1} - Avg Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "    # Save model checkpoint periodically\n",
        "    if (epoch + 1) % SAVE_EVERY == 0:\n",
        "        checkpoint_path = checkpoint_dir / f\"model_epoch_{epoch+1}.pt\"\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'ema_model_state_dict': ema_model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': avg_epoch_loss,\n",
        "            'epoch_losses': epoch_losses\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "    # Generate and save samples periodically\n",
        "    if (epoch + 1) % SAMPLE_EVERY == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            _, sample_mask = next(iter(loader))\n",
        "            sample_mask = sample_mask[:8].to(device)\n",
        "\n",
        "            samples = sample_cond(\n",
        "                ema_model,      # Use EMA model for better quality\n",
        "                scheduler,\n",
        "                mask=sample_mask\n",
        "            )\n",
        "\n",
        "            samples = (samples + 1) / 2  # Denormalize\n",
        "            save_image(\n",
        "                samples,\n",
        "                sample_dir / f\"samples_epoch_{epoch+1}.png\",\n",
        "                nrow=4\n",
        "            )\n",
        "            print(f\"Samples saved: samples_epoch_{epoch+1}.png\")\n",
        "\n",
        "        model.train()\n",
        "\n",
        "# Save final trained model\n",
        "final_path = checkpoint_dir / \"model_final.pt\"\n",
        "torch.save({\n",
        "    'epoch': NUM_EPOCHS,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'ema_model_state_dict': ema_model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'epoch_losses': epoch_losses\n",
        "}, final_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxxUv5bVvR4C"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This block generates 400 human images using the trained EMA diffusion model.\n",
        "We take masks from the DataLoader and generate one image per mask.\n",
        "Each generated image is saved as a separate PNG file to Google Drive.\n",
        "'''\n",
        "\n",
        "# Output directory on Drive\n",
        "out_dir = Path(\"/content/drive/MyDrive/SHHQ_1.0/SHHQ-1.0/model_samples\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_and_save_samples(\n",
        "    ema_model,\n",
        "    scheduler,\n",
        "    loader,\n",
        "    out_dir,\n",
        "    total_samples=400\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates `total_samples` images and saves them one-by-one.\n",
        "\n",
        "    Args:\n",
        "        ema_model: EMA version of the diffusion model (better quality).\n",
        "        scheduler: DDPM scheduler used for sampling.\n",
        "        loader: DataLoader that returns (image, mask).\n",
        "        out_dir: Directory to save generated PNG files.\n",
        "        total_samples: Number of images to generate.\n",
        "    \"\"\"\n",
        "    ema_model.eval()\n",
        "\n",
        "    saved = 0\n",
        "    loader_iter = iter(loader)\n",
        "\n",
        "    while saved < total_samples:\n",
        "        try:\n",
        "            _, mask = next(loader_iter)  # we only need the mask\n",
        "        except StopIteration:\n",
        "            # restart loader if it ends before reaching total_samples\n",
        "            loader_iter = iter(loader)\n",
        "            _, mask = next(loader_iter)\n",
        "\n",
        "        mask = mask.to(device)\n",
        "\n",
        "        # How many samples to generate from this batch\n",
        "        remaining = total_samples - saved\n",
        "        b = min(mask.size(0), remaining)\n",
        "\n",
        "        # Use only the needed number of masks\n",
        "        mask_batch = mask[:b]\n",
        "\n",
        "        # Generate images with DDPM sampling (mask-conditioned)\n",
        "        samples = sample_cond(ema_model, scheduler, mask=mask_batch)\n",
        "\n",
        "        # Convert from [-1, 1] to [0, 1] for saving\n",
        "        samples = (samples.clamp(-1, 1) + 1) / 2\n",
        "\n",
        "        # Save each image separately\n",
        "        for i in range(b):\n",
        "            save_path = out_dir / f\"sample_{saved:04d}.png\"\n",
        "            save_image(samples[i], save_path)\n",
        "            saved += 1\n",
        "\n",
        "        print(f\"Saved {saved}/{total_samples} images...\", end=\"\\r\")\n",
        "\n",
        "    print(f\"\\n Done! Saved {total_samples} images to: {out_dir}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaE4r4V6wKDZ"
      },
      "outputs": [],
      "source": [
        "# Run generation\n",
        "generate_and_save_samples(\n",
        "    ema_model=ema_model,\n",
        "    scheduler=scheduler,\n",
        "    loader=loader,\n",
        "    out_dir=out_dir,\n",
        "    total_samples=400\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
